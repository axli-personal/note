# 一致性

## 上下游业务结果一致性

淘宝团队分享了一个[积分兑换奖品业务一致性](https://tech.taobao.org/news/zwfxoz)的案例.

### 幂等

如果积分扣减1次, 但是奖品发了2次, 上下游业务结果肯定不一致, 会造成资损.

所以上游和下游业务处理一定要做好幂等, 确保用户一次操作, 在各链路中都不要重复处理.

* 客户端需要防止用户重复点击.
* 客户端请求需要携带请求唯一标识.
* 积分团队根据请求唯一标识做幂等, <u>积分变动表</u>需要存储唯一请求标识, 重复插入会因为索引冲突导致事物失败.
* 积分团队调用奖品团队接口时, 需要携带`积分变动表ID/客户端唯一请求标识`.
* 通过幂等ID发现请求重复之后, 只需要根据当前状态继续推进流程就可以了, 不需要重复操作.

#### 重试时校验上游参数

首次请求时, 上游关键参数需要记录快照, 防止重试时上游参数发生变化, 导致上下游业务结果不一致.

所以积分团队在插入积分变动记录时, 还需要将兑换奖品的`SKU ID`、兑换数量也记录到表里.

如果重试时关键参数发生变化, 可以给上游调用方报错.

#### 查询时保存结果快照

如果在重试的时候, 业务流程中查询到的一些结果发生了变化, 很有可能会导致最终业务结果发生变化.

对于一些关键数据的查询, 有必要记录一份结果快照. 在重试的时候, 可以使用之前快照中的数据.

#### 调用下游参数快照

如果在重试过程中, 调用下游的参数发生了变化, 很容易导致下游接口失败, 而违背幂等性.

#### 不需要快照的数据

如果数据在业务流程中不会发生变化, 或没有使用到, 可以不用存储到快照, 减轻存储压力.

### 状态机

处理业务流程时, 需要通过状态机完成状态的**有序推进**, 将业务操作和状态更新和放在一个事务中.

在少数简单业务场景下, 业务数据和状态数据维护在一张表, 可以采用乐观锁(CAS)的方式控制并发.

### 推进机制

* 调用下游接口成功后, 立刻做后续推进.
* 调用下游接口超时后, 在一定等待时间后, 主动查询处理结果.
* 下游接口执行完成之后, 将结果通过`MQ`推送给上游, 上游消费到结果后做后续推进.
* 通过定时任务, 主动对非终态的数据, 进行重试.

因为有多种推进机制, 一定要保证各机制获取到最终状态; 获取到非终态结果时, 仍需要继续重试.

如果通过不同的推进机制, 获取到了不同的终态结果, 那下游一定出现了逻辑错误/故障, 需要报警人工处理.

### 对账

* 实时对账: 通过监听BinLog, 在指定的状态转换发生时, 核对上下游团队的数据.
* 离线对账: 将上下游的数据同步到离线库, 通过任务核对一个周期内的数据.

### 容灾

对于很多业务是要求不能中断的, 各节点状态推进都依赖事务. 主库挂掉对业务流程影响非常大, DB需要有备主做容灾.

## 第三方数据一致性

美团酒店业务团队分享了一个[供应商数据缓存一致性](https://tech.meituan.com/2016/12/26/data-consistency.html)的案例.

### 业务背景

200供应商, 3万个酒店, 30万各房型(`Stock Keeping Unit`).

对接酒店供应商, 使得用户可以在美团和大众点评上直接预定房型, 预定范围为60天内.

将每个供应商的数据都同步到内部, 其中包括: 房型信息、库存、价格(60天内的)、预定规则.

数据同步之后, 就可以展示给C端的用户(推荐、搜索、详情), 同时也可以用于下单的流程.

数据同步最终肯定是以供应商的数据为准, 内部缓存的数据和供应商的数据一致性经常存在不一致.

### 从无到有

对于刚接入的供应商, 需要通过任务进行全量的数据同步. 任务的间隔应该尽可能的短, 以减少数据不一致的时间.

如果任务被调度时, 上一次任务还没有执行完成, 不能重复执行, 供应商的系统承受不了大量的查询.

### 冷热数据分治

大部分用户(95%)只会预定10天以内的房型, 所以10天内的热点数据的同步频率应该快于60天的全量数据, 热点15min, 全量每天4次.

除了时间维度的分治, 也可以通过一些推荐算法预测热点数据, 针对预测的热点制定不同的同步策略.

### 触发式更新

* 用户浏览房型的时候, 调用供应商接口提前同步最新的数据, 多次浏览需要做时间窗口的去除, 防止查询量过大.
* 用户下单校验流程中, 调用供应商的校验, 校验失败后异步同步改房型的数据, 下一次请求的时候, 就有最新的数据了.


### 推送增量数据

供应商调用公网的HTTP接口, 实时推送房型数据的变化.
